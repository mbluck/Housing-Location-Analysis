{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm \n",
    "from scipy import stats\n",
    "from geopy.distance import geodesic\n",
    "import itertools as it\n",
    "\n",
    "#web scraping\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import http.client, urllib.parse\n",
    "import lxml.html as lx\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate houses sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generates a sample of up to 500 houses from each city\n",
    "'cities' is a list of tuples where the first value is the city name and the second value is the state code\n",
    "Returns a dataframe\n",
    "'''\n",
    "def getHouses(cities):\n",
    "\n",
    "    houses_list = []\n",
    "\n",
    "    url = \"https://realty-mole-property-api.p.rapidapi.com/saleListings\"\n",
    "\n",
    "    for city, state in cities: #for each city, get up to 500 houses for sale\n",
    "        querystring = {\"city\":city,\"state\":state,\"limit\":\"500\"}\n",
    "\n",
    "        headers = {\n",
    "            \"X-RapidAPI-Key\": \"43686bd243mshe8a0be6f9e0556cp10a1bbjsn58033bf0f546\",\n",
    "            \"X-RapidAPI-Host\": \"realty-mole-property-api.p.rapidapi.com\"\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "        for house in response.json(): #get the info for each house\n",
    "            if type(house) == dict:\n",
    "                address = house.get('formattedAddress')\n",
    "                price = house.get('price')\n",
    "                city = house.get('city')\n",
    "                state = house.get('state')\n",
    "                sqf = house.get('squareFootage')\n",
    "                try:\n",
    "                    pricePerSqft = price / sqf\n",
    "                except:\n",
    "                    pricePerSqft = None\n",
    "                latitude = house.get('latitude')\n",
    "                longitude = house.get('longitude')\n",
    "                \n",
    "                row = {'Address':address, 'Price':price, 'City':city, 'State':state, 'SquareFootage':sqf, \n",
    "                       'PricePerFt': pricePerSqft, 'Latitude':latitude, 'Longitude':longitude}\n",
    "            \n",
    "            houses_list.append(row)\n",
    "\n",
    "    houses = pd.DataFrame.from_dict(houses_list)\n",
    "\n",
    "    #remove na and duplicates\n",
    "    houses.dropna(axis=0, inplace=True)\n",
    "    houses.drop_duplicates(subset=['Address', 'City'], keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "    return houses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#GENERATE HOUSES SAMPLE\n",
    "cities = [('Los Angeles', 'CA'), ('Anaheim','CA'), ('Long Beach', 'CA'), ('Chicago', 'IL'), ('Naperville', 'IL'), ('Elgin', 'IL'),\n",
    "             ('Dallas', 'TX'), ('Fort Worth', 'TX'), ('Arlington', 'TX'), ('Washington', 'DC'), ('Arlington', 'VA'), ('Alexandria', 'VA')]\n",
    "\n",
    "houses = getHouses(cities)\n",
    "\n",
    "houses.to_csv('houses.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate landmarks sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generates the landmarks from each city and stores them in a dataframe\n",
    "scrapes data from YellowPages.com\n",
    "cities is a list of tuples where the first value is the city name and the second value is the state code\n",
    "places is a list of landmark types\n",
    "'''\n",
    "\n",
    "def getPlaces(cities, places):\n",
    "\n",
    "    #create list of lists to hold all the data\n",
    "    data = [[],[],[],[],[]]\n",
    "\n",
    "    for city, state in cities: #for each city and each landmark type, get the yellow pages search results\n",
    "        for place in places:\n",
    "\n",
    "            addresses = []\n",
    "            names = []\n",
    "\n",
    "            for i in range(1, 3):\n",
    "                if i == 1:\n",
    "                    url = \"https://www.yellowpages.com/search?search_terms=\" + place + \"&geo_location_terms=\" + city + \"%2C+\" + state \n",
    "                else:\n",
    "                    url = \"https://www.yellowpages.com/search?search_terms=\" + place + \"&geo_location_terms=\" + city + \"%2C+\" + state + \"&page=\" + str(i)\n",
    "                time.sleep(0.05)\n",
    "                response = requests.get(url,\n",
    "                                    headers = {\"accept\" : \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "                                                \"accept-encoding\": \"gzip, deflate, br\",\n",
    "                                                \"accept-language\": \"en-US,en;q=0.9\",\n",
    "                                                \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36\"})\n",
    "\n",
    "                html = response.text\n",
    "                page = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "                #get content\n",
    "                addresses_raw = page.find_all(\"div\", class_=\"street-address\") #a list of all the addresses in the webpage\n",
    "                names_raw = page.find_all(\"a\", class_=\"business-name\") #a list of all the location/business names\n",
    "                \n",
    "                #extract address from html snippet\n",
    "                for address in addresses_raw:\n",
    "                    if address == '':\n",
    "                        continue\n",
    "                    else:\n",
    "                        address = str(address).split('>')[1] \n",
    "                        address = address.split('<')[0] \n",
    "                        addresses.append(address)\n",
    "                    \n",
    "\n",
    "                #extract name from html snippet\n",
    "                for name in names_raw:\n",
    "                    name = str(name).split('>')[2]\n",
    "                    name = name.split('<')[0]\n",
    "                    names.append(name)\n",
    "\n",
    "                #list of names may contain blank elements at the end, remove them\n",
    "                while len(names) > len(addresses):\n",
    "                    del names[-1]\n",
    "                \n",
    "                #consolidate all school types \n",
    "                #these three search terms were used instead of just \"school\" because they yielded more relevant results\n",
    "                if place in ['elementary school', 'middle school', 'high school']:\n",
    "                    place = 'school'\n",
    "\n",
    "                #add everything to the list of lists\n",
    "                data[0].extend([place] * len(names)) #add landmark type\n",
    "                data[1].extend(names) #add location/business names\n",
    "                data[2].extend(addresses) #add addresses\n",
    "                data[3].extend([city] * len(names)) #add city\n",
    "                data[4].extend([state] * len(names)) #add state \n",
    "\n",
    "    #now that we have all the data, turn list of lists into a dataframe\n",
    "    landmarks = pd.DataFrame(data).T   \n",
    "    landmarks.columns = ['Landmark', 'Name', 'Address', 'City', 'State']      \n",
    "\n",
    "    #remove na and duplicates\n",
    "    landmarks.dropna(axis=0, inplace=True)\n",
    "    landmarks.drop_duplicates(subset=['Name', 'Address'], keep='first', inplace=True, ignore_index=True)\n",
    "\n",
    "    return landmarks\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate sample\n",
    "cities = [('Los Angeles', 'CA'), ('Anaheim','CA'), ('Long Beach', 'CA'), \n",
    "          ('Chicago', 'IL'), ('Naperville', 'IL'), ('Elgin', 'IL'),\n",
    "          ('Dallas', 'TX'), ('Fort Worth', 'TX'), ('Arlington', 'TX'), \n",
    "          ('Washington', 'DC'), ('Arlington', 'VA'), ('Alexandria', 'VA')]\n",
    "\n",
    "places = ['hospital', \n",
    "          'grocery', \n",
    "          'park', \n",
    "          'beach', \n",
    "          'cemetary', \n",
    "          'shopping', \n",
    "          'restaurant', \n",
    "          'golf course', \n",
    "          'prison', \n",
    "          'worship',\n",
    "          'elementary school', \n",
    "          'middle school', \n",
    "          'high school']\n",
    "\n",
    "landmarks = getPlaces(cities, places)\n",
    "landmarks.to_csv('landmarks_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the landmark coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GET COORDINATES FOR LANDMARKS \n",
    "\n",
    "'''\n",
    "Converts addresses into gps coordinates.\n",
    "Takes \"landmarks.csv\" and writes two new columns ('Latitude' and 'Longitude') containing the coordinates.\n",
    "'''\n",
    "\n",
    "def getCoords(df):\n",
    "    \n",
    "    #api info\n",
    "    conn = http.client.HTTPConnection('api.positionstack.com')\n",
    "    key = '614513ff11a7392f2a8c5c2ed0f88cfa'\n",
    "    \n",
    "    '''\n",
    "    Helper function which sends a query to the api and returns the latitude and longitude as a tuple\n",
    "    'address' is a string in the form \"street address, city, state\"\n",
    "    '''\n",
    "    def query(address, state):\n",
    "\n",
    "        #api query parameters\n",
    "        params = urllib.parse.urlencode(\n",
    "            {\n",
    "            'access_key': key,\n",
    "            'query': address,\n",
    "            'region_code': state,\n",
    "            'country': 'US',\n",
    "            'limit': 1\n",
    "            })\n",
    "        \n",
    "        conn.request('GET', '/v1/forward?{}'.format(params)) #query\n",
    "        results = conn.getresponse() #json data\n",
    "        \n",
    "        #occasionally, the query returns unexpected content\n",
    "        #the try/except block labels these instances as Nan, making them easy to filter out of a dataframe\n",
    "        try:\n",
    "            data = json.loads(results.read())['data'][0] #extracts the info we want from json, 'data' is a dict\n",
    "            lat = data['latitude']\n",
    "            lon = data['longitude']\n",
    "            return (lat, lon)\n",
    "            \n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    #lists to store coordinates\n",
    "    latitude = []\n",
    "    longitude = []\n",
    "    \n",
    "    #to monitor progress\n",
    "    counter = 1\n",
    "    start_time = time.time()\n",
    "\n",
    "    #call query() for each address and store return values\n",
    "    for address, city, state in zip(df['Address'], df['City'], df['State']):\n",
    "        result = query(address + ', ' + city + ', ' + state, state)  \n",
    "        \n",
    "        if result is None:\n",
    "            latitude.append(None)\n",
    "            longitude.append(None)  \n",
    "        else:                                                                        \n",
    "            latitude.append(result[0])\n",
    "            longitude.append(result[1])\n",
    "\n",
    "        #display progress\n",
    "        if (counter % 2000) == 0:\n",
    "            print(f'progress: {counter}/{len(df)}   time: {time.time() - start_time} secs')\n",
    "        \n",
    "        counter += 1\n",
    "        time.sleep(1) #if queries are sent too quickly, false coordinates will be returned\n",
    "                      #unfortunately, this necessary delay means the function takes hours to finish\n",
    "\n",
    "    #add coordinates to dataframe\n",
    "    df['Latitude'] = latitude\n",
    "    df['Longitude'] = longitude\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: 2000/17540   time: 2376.1752648353577 secs\n",
      "progress: 4000/17540   time: 4746.536860466003 secs\n",
      "progress: 6000/17540   time: 7117.204186201096 secs\n",
      "progress: 8000/17540   time: 9484.64375281334 secs\n",
      "progress: 10000/17540   time: 11842.947708845139 secs\n",
      "progress: 12000/17540   time: 14202.179571390152 secs\n",
      "progress: 14000/17540   time: 16543.056074142456 secs\n",
      "progress: 16000/17540   time: 18892.102096796036 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nca_coords = getCoords(landmarks_ca)\\nca_coords.to_csv('ca_coords.csv', index=False, float_format=f'%.{6}f')\\n\\ndc_coords = getCoords(landmarks_dc)\\ndc_coords.to_csv('dc_coords.csv', index=False, float_format=f'%.{6}f')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmarks = pd.read_csv('landmarks.csv')\n",
    "\n",
    "landmarks_coords = getCoords(landmarks)\n",
    "\n",
    "landmarks_coords.to_csv('landmarks_coords.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the minimum distance from each house to each landmark type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(house, landmark):\n",
    "\n",
    "    def iterate_house(row):\n",
    "        landmark_types = ['hospital', 'cemetary', 'park', 'beach', 'shopping', 'grocery', 'restaurant', 'golf course', 'worship', 'school']\n",
    "        all_dist = []\n",
    "\n",
    "        coord1 = (row['Latitude'], row['Longitude'])\n",
    "\n",
    "        min_dist = np.empty(10)\n",
    "        for place in landmark_types:\n",
    "            places = landmark[landmark['Landmark']==place]\n",
    "            all_dist = []\n",
    "\n",
    "            latitude = places['Latitude'].tolist() \n",
    "            longitude = places['Longitude'].tolist()\n",
    "            \n",
    "            for lat, lon in zip(latitude, longitude):\n",
    "                coord2 = (lat, lon)\n",
    "                curr_dist = geodesic(coord1, coord2).miles\n",
    "                all_dist.append(curr_dist)\n",
    "\n",
    "            np.append(min_dist, min(all_dist))\n",
    "\n",
    "        return min_dist\n",
    "            \n",
    "    distances = house.apply(iterate_house, axis=1) \n",
    "    return distances\n",
    "\n",
    "    houses = [(house['Latitude'][i], house['Longitude'][i]) for i in range(len(house['Latitude']))]\n",
    "    landmarks = [(landmark['Latitude'][i], landmark['Longitude'][i]) for i in range(len(landmark['Latitude']))]\n",
    "\n",
    "    for house_coord in houses:\n",
    "        dist = []\n",
    "        for landmark_coord in landmarks:\n",
    "            curr_dist = geodesic(house_coord, landmark_coord).miles\n",
    "            dist.append(curr_dist)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "house = pd.read_csv('houses.csv', names=['Address', 'Price', 'City', 'State', 'SqFt', 'PricePerSqft', 'Latitude', 'Longitude'])\n",
    "house = house[house['State']=='TX']\n",
    "landmark = pd.read_csv('tx_coords.csv', names=['Landmark', 'Name', 'Address', 'City', 'State', 'Latitude', 'Longitude'])\n",
    "landmark.dropna(inplace=True)\n",
    "house_sample = house.head(5).reset_index(drop=True)\n",
    "landmark_sample = landmark.groupby('Landmark').head(5)[1:].reset_index(drop=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [1.2120567758953e-311, 1.212056775421e-311, 1....\n",
       "1    [3.873e-321, 3.937e-320, 3.725e-321, 9.027e-32...\n",
       "2    [1.2120175286484e-311, 1.2120175288856e-311, 1...\n",
       "3    [118.262785, 118.277756, 118.283209, 118.23708...\n",
       "4    [1.212015601587e-311, 1.212015601587e-311, 1.2...\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = test(house_sample, landmark_sample)\n",
    "distances"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
